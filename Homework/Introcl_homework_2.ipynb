{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2020, Erk\n",
    "\n",
    "# Homework 2: Loops, dictionaries, and language models\n",
    "\n",
    "## Due: Tuesday September 21, 3pm right before class\n",
    "\n",
    "## Your name: Eloragh Espie\n",
    "## Your EID: eae2273\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_2.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Before you do anything else, please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine.\n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Punctuation, with loops (10 pts.) \n",
    "\n",
    "\n",
    "Here is a text passage from H.G. Wells, *The War of the Worlds* (from Project Gutenberg, http://www.gutenberg.org/etext/36):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellstext = \"\"\"After the glimpse I had had of the Martians emerging \n",
    "from the cylinder in which they had come to the earth \n",
    "from their planet, a kind of fascination paralysed my actions.  \n",
    "I remained standing knee-deep in the heather, staring at the \n",
    "mound that hid them.  I was a battleground of fear and curiosity.\n",
    "\n",
    "I did not dare to go back towards the pit, but I felt a passionate\n",
    "longing to peer into it.  I began walking, therefore, in a big curve,\n",
    "seeking some point of vantage and continually looking at the sand\n",
    "heaps that hid these new-comers to our earth.  Once a leash of thin\n",
    "black whips, like the arms of an octopus, flashed across the sunset\n",
    "and was immediately withdrawn, and afterwards a thin rod rose up,\n",
    "joint by joint, bearing at its apex a circular disk that spun with a\n",
    "wobbling motion.  What could be going on there?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, please remove punctuation from beginnings and ends of words in the passage, but do not use regular expressions. Instead, please proceed as follows:\n",
    "\n",
    "* Please split the passage into words using Python's ```split()```. Do not use NLTK's ```word_tokenize()```, as that already does the work of removing punctuation for you. *(If there is already a tool to do this work for you, why should you practice how to do it by hand? For example because ```word_tokenize()``` may not work equally well for all languages, or because you may want to design a better ```word_tokenize()```.)*\n",
    "\n",
    "* Then run a for-loop over the list of words, and use Python's ```strip()``` method to remove punctuation. You will need to give `strip()` an appropriate parameter to do this. Collect the resulting list of words without punctuation in a separate list. (That is, use the \"aggregation\" programming idiom that we discussed in class.) You do not need to keep the punctuation in a separate string. For this problem please just You do not need to remove punctuation from the middle of strings, that is, you do not need to transform the string ``new-comers'' in any way. \n",
    "\n",
    "Here is information about the method ```strip()```: https://docs.python.org/3.8/library/stdtypes.html#str\n",
    "\n",
    "Also note that Python has a string full of punctuation symbols that might be useful for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the box for your punctuation removal code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'the',\n",
       " 'glimpse',\n",
       " 'I',\n",
       " 'had',\n",
       " 'had',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Martians',\n",
       " 'emerging',\n",
       " 'from',\n",
       " 'the',\n",
       " 'cylinder',\n",
       " 'in',\n",
       " 'which',\n",
       " 'they',\n",
       " 'had',\n",
       " 'come',\n",
       " 'to',\n",
       " 'the',\n",
       " 'earth',\n",
       " 'from',\n",
       " 'their',\n",
       " 'planet',\n",
       " 'a',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fascination',\n",
       " 'paralysed',\n",
       " 'my',\n",
       " 'actions',\n",
       " 'I',\n",
       " 'remained',\n",
       " 'standing',\n",
       " 'knee-deep',\n",
       " 'in',\n",
       " 'the',\n",
       " 'heather',\n",
       " 'staring',\n",
       " 'at',\n",
       " 'the',\n",
       " 'mound',\n",
       " 'that',\n",
       " 'hid',\n",
       " 'them',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'battleground',\n",
       " 'of',\n",
       " 'fear',\n",
       " 'and',\n",
       " 'curiosity',\n",
       " 'I',\n",
       " 'did',\n",
       " 'not',\n",
       " 'dare',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'pit',\n",
       " 'but',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'a',\n",
       " 'passionate',\n",
       " 'longing',\n",
       " 'to',\n",
       " 'peer',\n",
       " 'into',\n",
       " 'it',\n",
       " 'I',\n",
       " 'began',\n",
       " 'walking',\n",
       " 'therefore',\n",
       " 'in',\n",
       " 'a',\n",
       " 'big',\n",
       " 'curve',\n",
       " 'seeking',\n",
       " 'some',\n",
       " 'point',\n",
       " 'of',\n",
       " 'vantage',\n",
       " 'and',\n",
       " 'continually',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'the',\n",
       " 'sand',\n",
       " 'heaps',\n",
       " 'that',\n",
       " 'hid',\n",
       " 'these',\n",
       " 'new-comers',\n",
       " 'to',\n",
       " 'our',\n",
       " 'earth',\n",
       " 'Once',\n",
       " 'a',\n",
       " 'leash',\n",
       " 'of',\n",
       " 'thin',\n",
       " 'black',\n",
       " 'whips',\n",
       " 'like',\n",
       " 'the',\n",
       " 'arms',\n",
       " 'of',\n",
       " 'an',\n",
       " 'octopus',\n",
       " 'flashed',\n",
       " 'across',\n",
       " 'the',\n",
       " 'sunset',\n",
       " 'and',\n",
       " 'was',\n",
       " 'immediately',\n",
       " 'withdrawn',\n",
       " 'and',\n",
       " 'afterwards',\n",
       " 'a',\n",
       " 'thin',\n",
       " 'rod',\n",
       " 'rose',\n",
       " 'up',\n",
       " 'joint',\n",
       " 'by',\n",
       " 'joint',\n",
       " 'bearing',\n",
       " 'at',\n",
       " 'its',\n",
       " 'apex',\n",
       " 'a',\n",
       " 'circular',\n",
       " 'disk',\n",
       " 'that',\n",
       " 'spun',\n",
       " 'with',\n",
       " 'a',\n",
       " 'wobbling',\n",
       " 'motion',\n",
       " 'What',\n",
       " 'could',\n",
       " 'be',\n",
       " 'going',\n",
       " 'on',\n",
       " 'there']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# place your code here\n",
    "\n",
    "punc = string.punctuation\n",
    "\n",
    "wellstext_list = wellstext.split()\n",
    "\n",
    "lst = []\n",
    "\n",
    "for word in wellstext_list:\n",
    "    word = word.strip(punc)\n",
    "    lst.append(word)\n",
    "\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Stemming, with loops (15 pts.)\n",
    "\n",
    "In the previous homework, you used regular expressions and word boundaries, \\\\b, for stemming. We now revisit stemming, but split the text into a word list first and then iterate over the members of the word list. We'll use the same text passage as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lewistext = \"\"\"And Peter with his sword still drawn in his hand went with \n",
    "the Lion to the eastern edge of the hill-top.  There a beautiful \n",
    "sight met their eyes.  The sun was setting behind their backs.  \n",
    "That meant that the whole country below them lay in the evening \n",
    "light--forest and hills and valleys and, winding away like a \n",
    "silver snake, the lower part of the great river.  And beyond \n",
    "all this, miles away, was the sea, and beyond\n",
    "the sea the sky, full of clouds which were just turning rose colour\n",
    "with the reflection of the sunset.  But just where the land of Narnia\n",
    "met the sea--in fact, at the mouth of the great river--there was\n",
    "something on a little hill, shining.  It was shining because it was a\n",
    "castle and of course the sunlight was reflected from all the windows\n",
    "which looked towards Peter and the sunset; but to Peter it looked like\n",
    "a great star resting on the seashore.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, please perform stemming on this text, as follows:\n",
    "    \n",
    "* Split the text into words, this time using ```nltk.word_tokenize()```. This will split off the punctuation into separate strings. The result is a list of words.\n",
    "\n",
    "* Iterate over the words in the list, perform stemming on each one (or leave it the same): Please remove word endings that are \"ing\", \"ed\", \"ion\", or \"s\" -- see options below. Put the resulting string in a new list. After the for-loop, the result should be a list of words exactly as long as the original word list, but stemmed. So this is another instance of the \"aggregation\" programming idiom we discussed in class.\n",
    "\n",
    "You have different options for how to identify word endings: You can use either ```endswith()```, or ```re.search()```. If you use ```re.search()```, note that the end of the word is now the end of the string, so you can use the anchor ```$``` instead of ```\\b```.\n",
    "\n",
    "You also have different options for removing endings: You can use string slices, or you can use ```re.sub()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 182\n"
     ]
    }
   ],
   "source": [
    "# put your code here\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "lewistext_list = nltk.word_tokenize(lewistext)\n",
    "\n",
    "stemmed_list = []\n",
    "\n",
    "for word in lewistext_list:\n",
    "    if re.search(r\"(ing|ed|ion|s)$\", word) is not None:\n",
    "        word = re.sub(r\"([A-Za-z]*)(ing|ed|ion|s)$\", r\"\\1\", word)\n",
    "        stemmed_list.append(word)\n",
    "    else:\n",
    "        stemmed_list.append(word)\n",
    "\n",
    "print(len(stemmed_list), len(lewistext_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Building an n-gram language model (20 pts.)\n",
    "\n",
    "For this problem, you will build an n-gram language model. Use `MLE` from `nltk.lm`, as in the notebook \"n-gram language models with NLTK.\"\n",
    "\n",
    "Please proceed as follows:\n",
    "* Choose a particular book from which to build the language model: For this, go to Project Gutenberg, https://www.gutenberg.org/, and choose a book. Then download *the plain-text version* of the book.\n",
    "* Do the whole following pipeline twice, once for **bigram** language models and once for **trigram** language models. Each time, for n-grams of length n:\n",
    "  * Use padding, as appropriate for n-grams of length n.\n",
    "  * Compute \"everygrams\" up to lenght n, that is, use n-grams substituted with shorter n-grams.\n",
    "  * Train an nltk `MLE` language model.\n",
    "  * Use it to produce 100 words. (You can do this last step several times, and you will get new results each time. You can do this to explore your language model.)\n",
    "  \n",
    "In the text box below the code box, please comment on your result. How did the bigram model compare to the trigram model? How specific was the generated text to the style of the book you chose? Also please paste into your text answer your favorite text produced by your bigram, and your favorite text produced by your trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here\n",
    "\n",
    "import nltk\n",
    "\n",
    "the_medea = open(\"the_medea.txt\")\n",
    "\n",
    "medea = the_medea.read()\n",
    "\n",
    "medea_sents = nltk.sent_tokenize(medea)\n",
    "\n",
    "medea_words = nltk.word_tokenize(medea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information for all the following problems\n",
    "\n",
    "## Part-of-speech tags, POS tags for short\n",
    "\n",
    "Part-of-speech tagging means labeling words in a text as \"this is a noun\", \"this is a verb\", \"this is a preposition\", and so on. There are some texts that have been manually labeled (by human annotators) with POS-tags, among them the Brown corpus (see below). Automatic POS-tagging is an important NLP task. \n",
    "\n",
    "Usually, the POS-tags we see used are more fine-grained than just \"this is a verb\" versus \"this is a noun\", they may distinguish, for example, nouns in singular and plural (\"house\" versus \"houses\"), and verbs in base form, 3rd person singular, and past tense (\"stroll\", \"strolls\", \"strolled\"). \n",
    "\n",
    "Here is a list of the POS-tags we will use in the following homework problems: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used\n",
    "\n",
    "## Accessing the POS-tagged Brown corpus\n",
    "\n",
    "The Brown corpus is a collection of text pieces from different domains, selected to be about balanced in size across different text types. It comprises about 1 million words. Here is more information on the Brown corpus: https://en.wikipedia.org/wiki/Brown_Corpus\n",
    "\n",
    "In this and the following problems\n",
    "you will use the *news* part of the Brown Corpus in the Natural\n",
    "Language Toolkit, more precisely a version of this corpus with\n",
    "part-of-speech tags (POS tags). In order to do that, you will need to\n",
    "install the NLTK data (if you have not done this already). Do this as follows:\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "Then follow the further instructions at http://www.nltk.org/data\n",
    "\n",
    "Afterwards, you can access the POS-tagged *news* part of the Brown Corpus as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "brown_news = nltk.corpus.brown.tagged_words(categories=\"news\")\n",
    "\n",
    "brown_news[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a list of tuples, to  be more precise a list of\n",
    "pairs. The first member of each pair is a word, and the second member\n",
    "is its part-of-speech tag. \n",
    "\n",
    "A tuple in Python is almost the same as a list: It is a sequence of\n",
    "items, and it is written like a list, except with round brackets instead of straight brackets. You can access an item using its index, as in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytuple = ('The', 'AT')\n",
    "mytuple[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one difference is that you cannot change a tuple, so for example\n",
    "you cannot use ```append()``` on it. \n",
    "\n",
    "For all problems below, please map all words to lowercase in order not to distinguish \"The\" and \"the\". You can use the method ```lower()``` for this, as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Word frequency and tag frequency, with Python dictionaries (30 pts.)\n",
    "\n",
    " What is the most frequent word, and what is the most frequent POS-tag in the *news* section of Brown? \n",
    " \n",
    " Do *not* use the NLTK data structures ```nltk.FreqDist```\n",
    "and ```nltk.ConditionalFreqDist```, or the Python data type\n",
    "```Counter```, for this problem. Just use Python dictionaries,\n",
    "  along with the built-in function ```sorted()```:\n",
    "  \n",
    "  * Make a dictionary to keep counts of words. \n",
    "  * Iterate through the list of word/tag pairs. Extract the word from the pair, and use a Python dictionary to keep count of how often you meet each word.\n",
    "  * Now make a dictionary to keep counts of tags. Iterate through the list of word/tag pairs again, but this time extract the tags and count them.\n",
    "  * Use ```sorted()``` to sort each dictionary by its values, and find the entries with the highest values.\n",
    "\n",
    "\n",
    "This is similar to the exercise we did in class where, for each\n",
    "word appearing in a paragraph from *The Onion*, we counted how often\n",
    "it appeared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Word frequency and tag frequency, with NLTK's FreqDist (10 pts.)\n",
    "\n",
    "Please re-do the task from problem 4, but this time use the NLTK ```FreqDist``` data type: What is the most frequent word, and what is the most frequent POS-tag in the *news* section of Brown? This time, create one ```nltk.FreqDist``` object for counts of words, and one for counts of tags. Then use the FreqDist method ```max()``` to determine the most frequent word and the most frequent tag. (Please make sure you get the same answer as before.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Ambiguous words, with Python dictionaries (15 pts.)\n",
    "\n",
    "In this problem, you will use nltk's ``ConditionalFreqDist`` to determine which words in the Brown news corpus have multiple different tags: How many words are ambiguous, in the sense that they appear with at least two different POS tags? For example, the English word \"object\" is ambiguous in that it could be either a verb or a noun (though I do not know if it occurs in the Brown news corpus as both a verb and a noun). \n",
    " \n",
    "To solve this problem, build an nltk ``ConditionalFreqDist`` from word/tag pairs *again only using the news part of the Brown corpus*.  You can see an example of a `ConditionalFreqDist` object in the notebook on n-gram language models. Here is another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 3 conditions>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "pairs = [ (\"a\", \"X\"), (\"a\", \"Y\"), (\"b\", \"X\"), (\"c\", \"Y\")]\n",
    "\n",
    "# this makes a ConditionalFreqDist object\n",
    "cd = nltk.ConditionalFreqDist(pairs)\n",
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the different values seen as first of a pair are conditions\n",
    "cd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'X': 1, 'Y': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the entry for \"a\" is all the second-of-a-pair entries seen with \"a\".\n",
    "# It is a FreqDist object\n",
    "cd[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Y \n",
      "1 1 \n"
     ]
    }
   ],
   "source": [
    "# So we have a complex data type: \n",
    "# A ConditionalFreqDist contains many FreqDist objects,\n",
    "# one for each condition.\n",
    "# You can save the FreqDist for \"a\" in a variable.\n",
    "fda = cd[\"a\"]\n",
    "# Then all the methods that come with a FreqDist\n",
    "# are available with that object, for example\n",
    "# the method tabulate(), which pretty-prints\n",
    "# all the counts. (This is not the method you need,\n",
    "# it is just a demonstration of some FreqDist method.)\n",
    "fda.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your code to solve the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble solving this one, read on below.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "To solve this problem, proceed as follows:\n",
    "* Make a ConditionalFreqDist from all the word/tag pairs in the Brown news corpus\n",
    "* Then use the aggregator programming idiom: \n",
    "  * Make an empty variable to store all ambiguous words\n",
    "  * Iterate over all the conditions in the ConditionalFreqDist (conditions are words here)\n",
    "  * For each condition/word, retrieve its associated FreqDist.\n",
    "  * Determine how many different tags there are that have counts in this FreqDist.\n",
    "    If you need help remembering which function to use for this, run ```help(nltk.FreqDist)``` in a new code box.\n",
    "  * if there is more than one tag that has counts in this FreqDist, store this condition/word in your variable that holds all ambiguous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
