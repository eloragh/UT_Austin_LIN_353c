{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2022, Erk\n",
    "\n",
    "# Homework 5:  Logistic regression, and word similarities\n",
    "\n",
    "## Due: Wednesday October 19, 3pm right before class\n",
    "\n",
    "## Your name: Eloragh Espie\n",
    "## Your EID: eae2273\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_5.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "* `trainreviews.txt`, `trainlabels.txt`, `testreviews.txt`, `testlabels.txt`: Data to be used for the sentiment analysis classifier\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine. However, there is one command below (loading a gensim space) that may take a while, please plan for that. \n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Sentiment Analysis with Logistic Regression \n",
    "\n",
    "In this problem, you will do classification, again for sentiment analysis: You will train a classifier to tell apart positive and negative  reviews. But this time, you will not use Naive Bayes but Logistic Regression.\n",
    "\n",
    "## Training and test data\n",
    "\n",
    "The data we use is from http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html. They have reviews in four different areas; we only use the DVD movie reviews. We have split the reviews into a training set and a test set; the files are included with this homework. \n",
    "\n",
    "Here is how to read in the reviews, and their gold labels. We have used 1 for positive reviews, and 0 for negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training datapoints:  1800\n",
      "number of test datapoints:  199\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "trainingdocs = open(\"trainreviews.txt\").readlines()\n",
    "# training labels: convert to integers\n",
    "traininglabels = [int(v) for v in open(\"trainlabels.txt\").read().split()]\n",
    "\n",
    "# test data\n",
    "testdocs = open(\"testreviews.txt\").readlines()\n",
    "# test labels: convert to integer\n",
    "testlabels = [int(v) for v in open(\"testlabels.txt\").read().split()]\n",
    "\n",
    "print(\"number of training datapoints: \", len(trainingdocs))\n",
    "print(\"number of test datapoints: \", len(testdocs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Preprocessing (20 points)\n",
    "\n",
    "In the Jupyter Notebook on Naive Bayes with scikit-learn, you learned how to use sklearn's feature extraction datatypes to transform text data into the vectors of numbers that sklearn takes as input. \n",
    "\n",
    "For this problem, use again the text transformation datatype `CountVectorizer`. Please check the notebook on Naive Bayes on how to get an object of this type. To remind you, `CountVectorizer` takes as input a list of documents and turns them into a matrix of counts.\n",
    "\n",
    "In the Naive Bayes notebook, we used the `CountVectorizer` object with all its default values. But it can be customized. If you like, you can experiment with some settings, for example the `CountVectorizer` can also remove stopwords, or count bigrams instead of unigrams only. You can find information on this here:\n",
    "* Here is the documentation page: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "* Here is a tutorial: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "If you experiment with the settings, document clearly what settings you used and what they do.\n",
    "\n",
    "Use a `CountVectorizer` object to transform both the training and the test data from above. Please store the `CountVectorizer` object in a variable called `feature_extractor`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training and test (20 points)\n",
    "\n",
    "For this problem, make a logistic regression object using `sklearn`, train it on the training data, and apply it to the test data. \n",
    "\n",
    "This works exactly like for Naive Bayes, except that you need a `LogisticRegression` object, as described here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "You will need to import it from `sklearn.linear_model`. Please store the classifier itself in a variable called `lmodel`. And store the classification results for the test data in a variable called `result`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now make a list that pairs each prediction with the true label by using the function `zip()`. This function takes two lists of equal length and \"zips\" them into a list of pairs, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 100), ('b', 200), ('c', 300)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [\"a\", \"b\", \"c\"]\n",
    "list2 = [100, 200, 300]\n",
    "list(zip(list1, list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to use this command:\n",
    "\n",
    "`predicted_and_gold = list(zip(result, testlabels))`\n",
    "\n",
    "This will produce a list of pairs. The first member of each pair is the model's guess, and the second member of the pair is the true label.\n",
    "\n",
    "When you inspect the first elements of `predicted_and_gold` with the command\n",
    "`predicted_and_gold[:5]`\n",
    "you should see something like this:\n",
    "`[(1, 1), (1, 1), (1, 1), (1, 1), (0, 1)]`\n",
    "\n",
    "Please compute the *accuracy* of your model as the fraction of test datapoint where it got the prediction right:\n",
    "* Count the number of correct guesses: Iterate over the pairs of labels in `predicted_and_gold`. Whenever the predicted label is the same as the gold label, add one to the number of correct guesses.\n",
    "* Afterwards, you can compute the accuracy as the number of correct guesses divided by the number of test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inspecting weights (20 points)\n",
    "\n",
    "As we discussed in class, logistic regression learns a weight for each feature. Features with a high positive weight are the features that are most typical of positive reviews. Features with a high negative weight are the features that are most typical of negative reviews. \n",
    "\n",
    "For this problem, you will inspect the weights to see which features the model found particularly indicative. \n",
    "\n",
    "We will again use the function `zip()` that we used in the previous problem. We will also make use of the fact that a logistic regression model in sklearn stores a matrix of all its feature weights in `coef_` (with the underscore). The features are in the same order as in the list of feature names in `feature_extractor`. You can make a list of all feature/weight pairs with the following command:\n",
    "\n",
    "`words_and_weights= zip(feature_extractor.get_feature_names(), \n",
    "                       lmodel.coef_.tolist()[0])`\n",
    "                       \n",
    "Please inspect indicative features as follows:\n",
    "* Sort the list `words_and_weights` of word/weight pairs by the weights.\n",
    "* Display the 10 words with the highest positive weight.\n",
    "* Display the 10 words with the lowest negative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your findings: Do the features intuitively make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Words in semantic space \n",
    "\n",
    "In the rest of this homework, you will explore pre-computed word embeddings, context-based representations of word meaning. For this, you will need the following Python packages:\n",
    "\n",
    "* gensim, https://pypi.org/project/gensim/\n",
    "* matplotlib, https://matplotlib.org/users/installing.html\n",
    "* sklearn, https://scikit-learn.org/stable/install.html#installation-instructions\n",
    "* numpy, https://www.scipy.org/install.html\n",
    "\n",
    "\n",
    "The *gensim* package comes with a number of pre-computed embeddings. For this homework, we will use the smallest one to make sure you can run the homework on your computer. If you want to use pre-computed word embeddings for your own work, please use embeddings that have more dimensions and that provide a better fit to human similarity ratings! (As a rule of thumb, higher-dimensional vectors will often give you better meaning representations, but they will also be a larger dataset.)\n",
    "\n",
    "Here is the code that loads pre-computed GloVE embeddings that have been learned from Wikipedia and Gigaword, where each word is represented by a 50-dimensional vector. *Note*: The first time you execute the `load()` command, it downloads the embedding\n",
    "dataset and stores it on your computer. If you execute the `load()` command a second time, the data will be loaded locally from your computer, it is not downloaded a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the gensim package : \n",
    "# we only need the downloader\n",
    "import gensim.downloader as gensim_api\n",
    "gensim_api.info()[\"models\"].keys()\n",
    "\n",
    "# getting the smallest space\n",
    "space = gensim_api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure we have stored in `space` comes with a number of methods defined by Gensim. Here are some of the most important ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most similar words to 'dog': [('cat', 0.9218005537986755), ('dogs', 0.8513159155845642), ('horse', 0.7907583713531494), ('puppy', 0.7754921913146973), ('pet', 0.7724707722663879), ('rabbit', 0.7720813751220703), ('pig', 0.7490061521530151), ('snake', 0.7399188280105591), ('baby', 0.7395570874214172), ('bite', 0.7387937903404236)]\n",
      "\n",
      "Cosine similarity of 'cat' and 'dog': 0.9218005\n",
      "\n",
      "here is the vector for 'rhino': [-0.47675  -0.90334  -0.22451   0.24039   0.45649   0.10803  -0.99265\n",
      " -1.5533    1.6991    0.39065   0.75083  -0.029712  0.73762  -0.18474\n",
      "  0.42353   0.10064   0.53984   0.22226  -0.81498   0.4811   -1.0921\n",
      " -0.060861  0.11872  -0.35688  -0.55111   0.22513  -0.32886   0.22122\n",
      " -0.088048 -1.0982    0.62585  -0.19342  -0.068856  0.59042   0.11721\n",
      "  0.83193   0.54431  -1.8303   -0.36146  -0.59634  -0.37526  -0.10193\n",
      " -0.41323  -0.5087    0.53766   0.28345   0.10833  -0.050318 -0.6323\n",
      " -0.88698 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This retrieves the top N (in this case 10)\n",
    "# most similar words to ’dog ’ in this\n",
    "# embedding space , with cosine similarities to ’dog ’\n",
    "print(\"10 most similar words to 'dog':\", \n",
    "      space.most_similar (\"dog\" , topn = 10))\n",
    "print()\n",
    "\n",
    "# This computes the cosine similarity between # the vectors for ’cat’ and ’dog’\n",
    "print(\"Cosine similarity of 'cat' and 'dog':\",\n",
    "      space.similarity (\"cat\" , \"dog\"))\n",
    "print()\n",
    "\n",
    "# this returns the vector for the word ’rhino ’\n",
    "print(\"here is the vector for 'rhino':\",\n",
    "      space [\"rhino\"])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Inspecting nearest neighbors (20 points)\n",
    "\n",
    "Use the method `most_similar()`, as demonstrated above, to determine the 10 nearest neighbors of:\n",
    "* three nouns of your choice that have more than one sense (like \"bank\" or \"bat\")\n",
    "* three verbs of your choice\n",
    "* three adjectives of your choice\n",
    "* three words that describe professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note down at least three observations about the nearest neighbors that you computed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Visualization (5 points)\n",
    "\n",
    "Below is a piece of code that projects word embeddings to a two-dimensional space in a way that, as far as possible, preserves relative location between points. It then visualizes the given points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# This function takes as input a list of words,\n",
    "# and a gensim KeyedVectors object,\n",
    "# and uses Principal Component Analysis\n",
    "# to project all the words to a 2-dimensional space,\n",
    "# which is visualized using pyplot.\n",
    "def visualize_words(words, space):\n",
    "    # get the vectors for all the words,\n",
    "    # and store in an array\n",
    "    word_vectors = np.array([space[w] for w in words])\n",
    "    \n",
    "    # use Principal Component Analysis \n",
    "    # to reduce the vectors for all the words to\n",
    "    # only two coordinates each\n",
    "    # while retaining as much of the information\n",
    "    # as possible\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "\n",
    "    # set up the canvas\n",
    "    plt.figure(figsize=(6,6))\n",
    "    # add a scatter plot of the two-D embeddings\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    # add each of the words to the plot, a bit above and to the right\n",
    "    # of the 2-D dot it goes with\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "\n",
    "    # now show the canvas\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the code, call `visualize_words()` with two arguments: a list of words, and a space. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_words([\"dog\", \"cat\", \"mouse\", \"gerbil\", \"canary\"], space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *three* of the words you inspected in 2.1, make visualizions that include the word and its 10 nearest neighbors as determined in 2.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write down at least two observations about the visualizations you obtained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Computing cosine similarity (15 points)\n",
    "\n",
    "The cosine similarity of two vectors $\\vec a = \\langle a_1, \\ldots, a_n\\rangle$ and $\\vec b = \\langle b_1, \\ldots, b_n\\rangle$ is defined as \n",
    "\n",
    "$cos(\\vec a, \\vec b) = \\frac{\n",
    "\\sum_i a_i b_i}{\\sqrt{\\sum_i a_i^2} \\sqrt{\\sum_i b_i^2}}$\n",
    "\n",
    "That is, for the numerator we get: $a_1b_1 + a_2b_2 + \\ldots + a_nb_n$. \n",
    "In the denominator, we get this for $\\vec a$: $\\sqrt{a_1a_1 + a_2a_2 + \\ldots + a_na_n}$, and anlogously for $\\vec b$. \n",
    "\n",
    "Please compute the cosine similarity by hand for the following two vectors, and show your work: \n",
    "\n",
    "* $\\langle 5, 12, 2\\rangle$\n",
    "* $\\langle 7, 2, 9\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write Python code that will compute cosine similarity, without using gensim's `similarity`. You will need to load the `math` package and use `math.sqrt()` for the square root. Apply your code to the two vectors from above.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c27904bfabd9e4a7f1d5f5dcc5d2a6d0fab6e51da12729af85e6eafdbb7b7a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
