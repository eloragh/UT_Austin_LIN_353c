{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2022, Erk\n",
    "\n",
    "# Homework 3:  Language models, and Nearest Neighbor classifiers\n",
    "\n",
    "## Due: Wednesday October 5, 3pm right before class\n",
    "\n",
    "## Your name:\n",
    "## Your EID:\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_3.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Before you do anything else, please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine.\n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Trigram language models (10 pts.)\n",
    "\n",
    "The Markov assumption says that when determining the probability of the next word in long sequence of items, as in \n",
    "\n",
    "$P(w_{10,001}|w_1, w_2, w_3, w_4, \\ldots , w_{10,000})$\n",
    "\n",
    "it is not the whole sequence that matters; we can approximate this probability by looking only at a few items directly preceding $w_{10,001}$. A bigram language model makes the assumption that it suffices to look at one directly preceding item. A trigram language model assumes that one only needs to look at the two directly preceding items, that is \n",
    "\n",
    "$P(w_{10,001}|w_1, w_2, w_3, w_4, \\ldots , w_{10,000}) \\approx P(w_{10,001}|w_{9,999}, w_{10,000})$\n",
    "\n",
    "A bigram language model estimates the probability of the word sequence\n",
    "\n",
    "    START I know END\n",
    "\n",
    "as\n",
    "\n",
    "$P(START\\:I\\:know\\:END) \\approx P(I|START)\\cdot P(know|I)\\cdot P(END|know)$\n",
    "  \n",
    "(where we omit the $P(START)$ in the beginning because we assume that the probability of starting with START is one.)\n",
    "\n",
    "Now assume the word sequence is\n",
    "\n",
    "    START My iguana is on fire END\n",
    "\n",
    "How does a trigram language model estimate the probability of this word sequence? That is, how is \n",
    "\n",
    "$P$(START My iguana is on fire END)\n",
    "    \n",
    "approximated by a trigram language model? Write down a formula analogously to the one for “I know” above, but for a trigram language model and for the given word sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(click here to insert for your text answer here)*\n",
    "\n",
    "# *CHECK*\n",
    "\n",
    "$P(START\\:My\\:iguana\\:is\\:on\\:fire\\:END) \\approx P(my | \\_, START) \\cdot P(iguana|START, my) \\cdot P(is| my, iguana) \\cdot P(on |iguana, is) \\cdot P(fire| is,on) \\cdot P(END | on, fire)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Estimating trigram probabilities (10 pts.)\n",
    "\n",
    "In class we have discussed that to estimate probabilities like $P(know|I)$ from data, we often use Maximum Likelihood Estimation, which in this case means that we estimate the **probability as relative frequency**.\n",
    "For a bigram language model, this means\n",
    "\n",
    "$P(know|I) = \\frac{count(I, know)}{count(I, \\_)}$\n",
    "\n",
    "For a trigram language model, how would you estimate \n",
    "\n",
    "$P(fire|is, on)$\n",
    "\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Click here to insert your text answer.)*\n",
    "\n",
    "# CHECK\n",
    "\n",
    "$P(fire |is, on) = \\frac{count(is, on, fire)}{count(is, on)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Understanding language models (15 pts.)\n",
    "\n",
    "You turn on the radio as it is broadcasting an interview. Assuming a trigram model, match up expressions (A), (B), (C) with descriptions (1), (2), (3):\n",
    "\n",
    "The expression\n",
    "\n",
    "  A. $P(Do)\\cdot P(you|Do)\\cdot P(think|Do you)$\n",
    "  \n",
    "  B. $P(Do|START)\\cdot P(you|START~Do)\\cdot P(think|Do~ you) \\cdot P(END|you~ think)$\n",
    "\n",
    "  C. $P(Do|START)\\cdot P(you|START~ Do)\\cdot P(think|Do~ you)$\n",
    "    \n",
    "represents the probability that\n",
    "\n",
    "  1. the first complete sentence you hear is “Do you think” (as in, “D’ya think?”)\n",
    "\n",
    "  2. the first 3 words you hear are “Do you think”\n",
    "\n",
    "  3. the first complete sentence you hear starts with “Do you think”\n",
    "  \n",
    "Explain your answers briefly.\n",
    "\n",
    "*This problem is from Jason Eisner’s homework on language modeling.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Click here to insert your text answer)*\n",
    "\n",
    "1 is B\n",
    "\n",
    "2 is A\n",
    "\n",
    "3 is C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Maximum likelihood estimation (25 pts.)\n",
    "\n",
    "In the jupyter notebook on n-gram language models, linked from the class webpage, we used Python code to compute bigram probabilities \n",
    "$P(word_2|word_1)$ \n",
    "as follows: First, count word bigram frequencies using ```nltk.ConditionalFreqDist()```, second, convert counts to probabilities using ```nltk.ConditionalProbDist()```.\n",
    "In this problem, you will do these two steps by hand.\n",
    "\n",
    "For the first step, you need to store counts that depend on two words, word1 and word2. In Python, you can do that with a \"Russian doll data structure\": a dictionary in which the values are again dictionaries. We have a dictionary with an entry for each word1. The entry for word1 then needs to map any possible word2 to the count of “word1 word2”, so it is a dictionary again. That is, we have a dictionary that maps a word to a dictionary that maps a second word to a count. You do not need to do step 1 -- it is given in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START I 2\n",
      "START Sam 1\n",
      "I am 2\n",
      "I do 1\n",
      "am Sam 1\n",
      "am END 1\n",
      "Sam END 1\n",
      "Sam I 1\n",
      "END START 2\n",
      "do not 1\n",
      "not like 1\n",
      "like green 1\n",
      "green eggs 1\n",
      "eggs and 1\n",
      "and ham 1\n",
      "ham END 1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "corpus = \"\"\"\n",
    "START I am Sam END\n",
    "START Sam I am END\n",
    "START I do not like green eggs and ham END\n",
    "\"\"\"\n",
    "\n",
    "words = corpus.split()\n",
    "# nltk .bigrams(words) yields a list of pairs , \n",
    "# [(’START’, ’I’), (’I’, ’am’), ...]\n",
    "# I can iterate over them by saying\n",
    "# ’for pair in nltk.bigrams(words):... ’\n",
    "# Or I can use assignment to multiple\n",
    "# variables at once, and that is what I will do.\n",
    "# As a reminder , assignment to multiple variables \n",
    "# looks like this :\n",
    "# word1, word2 = (’I’, ’am’)\n",
    "# This will assign ’I ’ to word1 and ’am’ to word2. \n",
    "# I am just doing this in a for−loop.\n",
    "\n",
    "# This is the aggregator 'idiom' of programming again:\n",
    "# initialize the aggregator variable called count\n",
    "# to be an empty dictionary,\n",
    "# then iterate over the list (of bigrams)\n",
    "# and update the aggregator\n",
    "count = { }\n",
    "for word1, word2 in nltk.bigrams(words): \n",
    "    if word1 not in count:\n",
    "        count[word1] = { }\n",
    "    if word2 not in count[word1]: \n",
    "        count[word1][word2] = 1\n",
    "    else :\n",
    "        count[word1][word2] = count[word1][word2] + 1\n",
    "        \n",
    "# This printout just demonstrates\n",
    "# that the code does the right thing\n",
    "for word1 in count :\n",
    "    for word2 in count[ word1 ]:\n",
    "        print(word1, word2, count[word1][word2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, extend the program to compute probabilities using Maximum Likelihood Estimation: Use the ```count``` dictionary with the computed frequencies from the “Sam corpus”, and compute another dictionary ```prob``` that will contain the estimated probability for each bigram. \n",
    "\n",
    "It should map a word word1 to a dictionary that maps a word word2 to the probability $P(word2|word1)$. As a reminder, the Maximum Likelihood estimate of, say, the probability of \"Sam\" given \"am\" is \n",
    "\n",
    "$P(Sam|am) = \\frac{count(am, Sam)}{count(am, \\_)}$\n",
    "\n",
    "where the denominator is the sum of the counts of all pairs starting in \"am\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'START': {'I': '67%', 'Sam': '33%'},\n",
       " 'I': {'am': '67%', 'do': '33%'},\n",
       " 'am': {'Sam': '50%', 'END': '50%'},\n",
       " 'Sam': {'END': '50%', 'I': '50%'},\n",
       " 'END': {'START': '100%'},\n",
       " 'do': {'not': '100%'},\n",
       " 'not': {'like': '100%'},\n",
       " 'like': {'green': '100%'},\n",
       " 'green': {'eggs': '100%'},\n",
       " 'eggs': {'and': '100%'},\n",
       " 'and': {'ham': '100%'},\n",
       " 'ham': {'END': '100%'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# place your code here\n",
    "\n",
    "#probability that am will be followed by sam = \n",
    "# P(Sam|am) =\n",
    "#count(am, Sam)/count(am, _)\n",
    "#the count of how many times \"am Sam\" has occured divided by the number of of pairs starting in am\n",
    "\n",
    "# create a dict to show probability of bigram occurences\n",
    "prob_dict = {}\n",
    "\n",
    "# loop through the first layer of the count dict\n",
    "for keys1, values in count.items():\n",
    "    # loop through the second layer, the dictionary within the keys of the count dict\n",
    "    for key, value in values.items():\n",
    "        # each key in count is unique so we don't need to check if they're already in the dictionary\n",
    "        # the formula here is a comprehension\n",
    "        # key = (count(first layer key, second layer key)/count(sum of occurences starting in first layer key))\n",
    "        prob_dict[keys1] = {key: (\"{:.0%}\".format((count[keys1][key]/sum([num for num in count[keys1].values()])))) for key in values}\n",
    "\n",
    "prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Smoothing (20 pts.)\n",
    "\n",
    "Some word sequences are rare, for example the sequence \"My iguana is on fire\", and even the initial \"my iguana\" may be so rare that in a corpus, you may never have seen the combination. In that case, a bigram language model would assign the sequence \"my iguana is on fire\" the probability zero, because $P(iguana|my) \\approx \\frac{count(my, ~iguana)}{count(my, ~\\_)} =  0$. \n",
    "\n",
    "However, \"my iguana is on fire\" is still a valid sequence of English words. \n",
    "\n",
    "There is a technique that is standardly used to fix this problem, called \"smoothing\". The idea is: Let's make it so that the probability of $P(iguana|my)$ is not zero but a small nonzero number, say 0.0001. Then the probability of the overall sequence will not be zero. \n",
    "\n",
    "But if we view the probability mass of 1 as a whole cake, then we had previously divided up the whole cake among word pairs that we have actually seen in the corpus. If we want to give \"iguana|my\" a small slice of the cake, there is some event that's going to get less cake, a.k.a. probbility. So in smoothing, we give a bit less probability mass to all observed events (where an event is here a bigram), and distribute the extra probability mass among unseen events. \n",
    "\n",
    "The simplest smoothing strategy is \"add-one smoothing\", where we simply \"hallucinate\" an additional count of one for every event. So events that were previously unseen now have a count of one, and events that were previously seen have an extra count of one. Since we have hallucinated some extra corpus data, we need to put that extra data into the denominator of the MLE estimation so that probabilities overall add up to one again. See the Jurafsky and Martin book 3rd edition, chapter 3.4, for a discussion of this smoothing regime: https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "\n",
    "For unigrams (one word at a time), if the unsmoothed MLE probability is\n",
    "\n",
    "$P(w) = \\frac{count(w)}{N}$\n",
    "\n",
    "where $N$ is the sum of all word counts, then its add-one smoothed probability is \n",
    "\n",
    "$P(w) = \\frac{count(w) + 1}{N + V}$\n",
    "\n",
    "where $V$ is the number of distinct words: We added a count of 1 for each word, for a hallucinated corpus of size $V$ overall. \n",
    "\n",
    "For bigrams, if the unsmoothed conditional probability is\n",
    "\n",
    "$P(w_2|w_1) = \\frac{count(w_1 \\:w_2)}{count(w_1 \\:\\_)} = \\frac{count(w_1\\:w_2)}{\\sum_w count(w_1\\:w)}$\n",
    "\n",
    "then for the smoothed probability, we are adding a count of 1 for each bigram:\n",
    "\n",
    "$P(w_2|w_1) = \\frac{count(w_1\\:w_2) + 1}{\\sum_w (count(w_1\\: w) + 1)} = \\frac{count(w_1\\:w_2) + 1}{count(w_1\\:\\_) + V}$\n",
    "\n",
    "where $V$ is again the size of the vocabulary. \n",
    "\n",
    "Re-using the dictionary ```count``` of bigram counts in the \"Sam\" corpus, compute a new dictionary ```prob_s``` of smoothed MLE probabilities. Inspect the result: It will differ drastically from ```prob``` because our corpus is so small. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Nearest neighbor classification (20 pts.)\n",
    "\n",
    "A nearest neighbor classifier is a classifier: it has a pre-defined list of labels, and given a piece of data $d$, it tags $d$ with one of the labels. In particular, a nearest neighbor classifier remembers all the datapoints $d_1, d_2, d_3, ...$ that is has seen during training, and their labels. When it now sees $d$, it determines the *nearest neighbor* of $d$ among the training datapoints, say $d_2$, and then labels $d$ with the same label as $d_2$. \n",
    "\n",
    "For this problem, you will do a bit of nearest neighbor classification, first by hand, then automatically. \n",
    "\n",
    "Here is some data. Say we have a collection of articles, each of which has a document class, shown in the last column below. Say we have also extracted keywords from each document, shown as key1...key4 below:\n",
    "\n",
    "| Document | key1 | key2 | key3 | key4 | Class |\n",
    "| --- | --- | --- | --- | --- | ---|\n",
    "| Doc1 | baseball | startups | college | soccer | Sports |\n",
    "| Doc2 | opera | training | opera | training | Art |\n",
    "| Doc3 | soccer | tech | AI | robots | AI | Tech | \n",
    "| Doc4 | sculpture | college | AI | baseball | exhibition | Art |\n",
    "| Doc5 | AI | exhibition | college | opera | Education | \n",
    "| Doc6 | startups | training | sculpture | college | Sports |\n",
    "\n",
    "## Part a: Nearest neighbors by hand\n",
    "\n",
    "There are different ways to define what the \"nearest neighbor\" should be. One simple way is to count how many keywords two documents have in common (ignoring duplicates). \n",
    "\n",
    "Using that method, what are the nearest neighbors of documents 1, 2, 3? If there are multiple nearest neighbors at the same distance, please list all of them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b: Nearest neighbors, with Python\n",
    "\n",
    "We do the same task again, but this time we use Python, specifically we usen ``NearestNeighbors`` from ``sklearn.neighbors``. This is one of those very complex data types that come with a lot of methods of their own, similar to NLTK's ``MLE`` language model data type.\n",
    "\n",
    "Here is an example of how to use ``NearestNeighbors``: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/eloraghespie/Documents/GitHub/LING_353c_COMP_LING/Homework/Introcl_homework_3.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LING_353c_COMP_LING/Homework/Introcl_homework_3.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m NearestNeighbors\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LING_353c_COMP_LING/Homework/Introcl_homework_3.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# three datapoints.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LING_353c_COMP_LING/Homework/Introcl_homework_3.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# datapoint1: has feature1 but not feature2, feature3\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eloraghespie/Documents/GitHub/LING_353c_COMP_LING/Homework/Introcl_homework_3.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m datapoint1 \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# three datapoints.\n",
    "# datapoint1: has feature1 but not feature2, feature3\n",
    "datapoint1 = [1, 0, 0]\n",
    "# datapoint2: has features 1 and 3\n",
    "datapoint2 = [1, 0, 1]\n",
    "# datapoint3: has features 2 and 3\n",
    "datapoint3 = [ 0, 1, 1]\n",
    "data = [ datapoint1, datapoint2, datapoint3]\n",
    "\n",
    "# fitting a nearest neighbor data structure\n",
    "# we tell it to compute 2 neighbors each,\n",
    "# as each datapoint is its own\n",
    "# nearest neighbor.\n",
    "# We use city block distance,\n",
    "# which here means: \n",
    "#  number of features that the two data points\n",
    "#  do NOT have in common (distance!)\n",
    "neighbor_obj = NearestNeighbors(n_neighbors=2, metric='cityblock')\n",
    "neighbor_obj.fit( data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest datapoint is datapoint no. 0 at distance 0.0 (which is that same datapoint)\n",
      "Second nearest datapoint is datapoint no. 1 at distance 1.0\n"
     ]
    }
   ],
   "source": [
    "# asking about the first nearest neighbor.\n",
    "# it gives us two answers: \n",
    "# first, pairwise distances (rather than similarities)\n",
    "# second, nearest neighbors.\n",
    "# It gives us two nearest neighbors, as we\n",
    "# set n_neighbors = 2\n",
    "distances, neighbors = neighbor_obj.kneighbors([[1,0,0]])\n",
    "\n",
    "print(\"Nearest datapoint is datapoint no.\", neighbors[0][0], \"at distance\", distances[0][0], \"(which is that same datapoint)\")\n",
    "print(\"Second nearest datapoint is datapoint no.\", neighbors[0][1], \"at distance\", distances[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datapoint no. 0 is: [1, 0, 0]\n",
      "Its nearest neighbor that is not itself is 1\n",
      "at distance 1.0\n",
      "datapoint no. 1 is: [1, 0, 1]\n",
      "Its nearest neighbor that is not itself is 0\n",
      "at distance 1.0\n",
      "datapoint no. 2 is: [0, 1, 1]\n",
      "Its nearest neighbor that is not itself is 1\n",
      "at distance 2.0\n"
     ]
    }
   ],
   "source": [
    "# We can determine nearest neighbors for all datapoints at once\n",
    "distances, neighbors = neighbor_obj.kneighbors(data)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"datapoint no.\", i, \"is:\", data[i])\n",
    "    print(\"Its nearest neighbor that is not itself is\", neighbors[i][1])\n",
    "    print(\"at distance\", distances[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, over to you: Please encode the six documents from above as lists of 0's and 1's, where 1 means that this document has the keyword, and 0 means it does not have it. Just encode the keywords, not the document number (Doc1, Doc2, ...) or the Class (Sports, Art, Tech, Education).\n",
    "\n",
    "Here, for your convenience, is the list of all keywords:\n",
    "\n",
    "    AI, baseball, college, exhibition, opera, robots, sculpture, soccer, startups, tech, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again make a NearestNeighbors object, specifying 2 neighbors, and cityblock metric. Fit it with the six datapoints.\n",
    "Then, for each datapoint, determine its nearest neighbor, as demonstrated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra credit, 5 points**: Some of the documents have duplicate keywords. Re-encode the documents as lists of numbers, but this time don't just use 0's and 1's, but the number of times each keyword appears in a document. Does that change the nearest neighbor for any document? How does this change the distances? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
