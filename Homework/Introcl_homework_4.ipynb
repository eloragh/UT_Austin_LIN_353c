{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2022, Erk\n",
    "\n",
    "# Homework 4:  Naive Bayes, and Classification\n",
    "\n",
    "## Due: Wednesday October 12, 3pm right before class\n",
    "\n",
    "## Your name: Eloragh Espie\n",
    "## Your EID: eae2273\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_4.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "* wsd_train.txt: training data\n",
    "* wsd_test.txt: test data\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Before you do anything else, please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine.\n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Word Sense Disambiguation by hand (40 points)\n",
    "\n",
    "Many words have different meanings, depending on their context. For example, in \n",
    "\n",
    "    They went to the *bank* to get money.\n",
    "    \n",
    "the word \"bank\" means a financial institution, but in \n",
    "\n",
    "    They sat on the *bank*, fishing.\n",
    "    \n",
    "the word \"bank\" means the slope at the side of a river.\n",
    "Some dictionaries list all the different senses that a word can take on. In computational linguistics, the WordNet dictionary is often used because it is available electronically. (In fact, NLTK has an interface to WordNet, but we are not using it for the current homework.)\n",
    "For example, here is the list of WordNet senses for the word \"mouse\": http://wordnetweb.princeton.edu/perl/webwn?c=7&sub=Change&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&i=-1&h=000000&s=mouse  It lists four noun senses, `mouse#1`, `mouse#2`, `mouse#3` and `mouse#4`, and two verb senses.\n",
    "\n",
    "This problem asks you to do some manual word sense annotation, and to comment on your observations. Please record your annotation in the box below. \n",
    "\n",
    "\n",
    "**Data.** Below, you find sentences from the SemCor corpus, all featuring the verb \"leave\". Look up the WordNet senses of the verb \"leave\" at http://wordnetweb.princeton.edu/perl/webwn?s=leave&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=1&o5=&o9=&o6=&o3=&o4=&h=000000 **using only the verb senses of the word**.\n",
    "\n",
    "For each of the sentences below, choose one best fitting Word-Net sense for the verb \"leave\" in that sentence, and record your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentences to annotate: Please record the sense number of your choice in this box, below each sentence.\n",
    "\n",
    "*(a) But questions with which committee members taunted bankers appearing as witnesses **left** little doubt that they will recommend passage of it .*\n",
    "\n",
    "Sense: 6\n",
    "\n",
    "*(b) The departure of the Giants and the Dodgers to California **left** New York with only the Yankees .*\n",
    "\n",
    "Sense: 11\n",
    "\n",
    "*(c) After the coach listed all the boy ’s faults , Hartweger said , “ Coach before I **leave** here , you ’ll get to like me ” .*\n",
    "\n",
    "Sense: 5\n",
    "\n",
    "*(d) R. H. S. Crossman , M.P. , writing in The Manchester Guardian , states that departures from West Berlin are now running at the rate not of 700 , but of 1700 a week , and applications to **leave** have risen to 1900 a week .*\n",
    "\n",
    "Sense: 8\n",
    "\n",
    "*(e) The house has been swept so clean that contemporary man has been **left** with no means , or at best with wholly inadequate means , for dealing with his experience of spirit .*\n",
    "\n",
    "Sense: 6\n",
    "\n",
    "*(f) A second and also good practice is to shear off the tops , **leaving** an inch high stub with just a leaf or two on each branch .*\n",
    "\n",
    "Sense: 11\n",
    "\n",
    "*(g) No doubt some experiences vanish so completely as to **leave** no trace on the sleeper ’s mind .*\n",
    "\n",
    "Sense: 11\n",
    "\n",
    "*(h) He is a widower , his three children are dead , he has no one **left** on earth ; also he is a drunk , and has lost his job on that account .*\n",
    "\n",
    "Sense: 12\n",
    "\n",
    "*(i) Piepsam tries to stop him by force , receives a push in the chest from “ Life ” , and is **left** standing in impotent and growing rage , while a crowd begins to gather .*\n",
    "\n",
    "Sense: 3\n",
    "\n",
    "*(j) The audience **leaves** the play under a spell , It is the kind of spell which the exposure to spirit in its living active manifestation always evokes .*\n",
    "\n",
    "Sense: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect on your annotation work \n",
    "\n",
    "Which sentences were hard to annotate, and why?\n",
    "\n",
    "Which pairs (or groups) of WordNet senses did you find hard to distinguish, and what criteria did you use to distinguish between them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*\n",
    "\n",
    "I marked 2 sentences as sense 11 and all of those were pretty difficult to to annotate. I felt like each had a different meaning, but ultimately sense 11 was the only one that seemed to fit them both.\n",
    "\n",
    "Sense 1, 5, and 8 all had very similar meanings related to leaving or exiting a physical place or situation. For sense 1, I differentiated it by the impacted the example sentences seemed to have relative to 5 or 8. \n",
    "\n",
    "Sense 1 sentences seemed more like passing thoughts and they were all relative to time rather than place - **\"The ship leaves at midnight\"**. \n",
    "\n",
    "I thought that sense 5 and sense 1 were almost identical at first, but I realized that sense 5 sentences were relative to a exiting from a space rather than at a time - **\"the fugitive has left the country\"**. \n",
    "\n",
    "For sense 8, I believe it has two distinguishing features from 1 and 5 - it can be relative to a space, time, or situation simultaneously and it can express some kind of desire. For example:\n",
    "\n",
    "- **\"He left the Senate after two terms\"** \n",
    "\n",
    "    He is leaving at specific time, after a specific time, or after a specific period of time and he is leaving a specific situation.\n",
    "- **\"She wants to leave\"**\n",
    "\n",
    "    A desire to exit a place or situation is expressable through this sense.\n",
    "\n",
    "Sentence d is the only sentence I marked as 8. I did this because the applications to leave are specific to a place - West Berlin - and the increase in applications expresses a broader desire to move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Naive Bayes: Computing the probability of each target class by hand (10 points)\n",
    "\n",
    "The file `wsd_train.txt` distributed with this homework contains word sense annotation data for the lemma feel. There is data for two Word- Net senses of feel:\n",
    "\n",
    "  * “experience”: to undergo an emotional sensation or be in a par- ticular state of mind. As in: “She felt happy”; “He felt regret”\n",
    "  * “think”: to come to believe on the basis of emotion, intuitions, or indefinite grounds. As in: “I feel that they like me”\n",
    "  \n",
    "Each line starts with a word that is the sense label, either `EXPERIENCE` or `THINK`. After that comes the sentence. Here is the first line as an example:\n",
    "\n",
    "    EXPERIENCE  I began to *feel* some guilt\n",
    "    \n",
    "This says that the sense label for this item is `EXPERIENCE`, and that this item contains the features I, began, to, some, guilt. The target word is encased in stars so you can omit it from feature counts.\n",
    "\n",
    "For this problem, please compute the overall probabilities of the two senses, THINK and EXPERIENCE, from the training data, that is, please compute $P(THINK)$ and $P(EXPERIENCE)$. Use Maximum Likelihood Estimation, that is, estimate the probabilities to be the same as relative frequencies in the training data. **For this problem, please do this by hand.** Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*\n",
    "\n",
    "# CHECK\n",
    "\n",
    "$P(THINK) = \\frac{count(THINK)}{count(THINK) + count(EXPERIENCE)}$\n",
    "\n",
    "$P(THINK) = \\frac{12}{12 + 15}$\n",
    "\n",
    "$P(THINK) = \\frac{4}{9}$\n",
    "\n",
    "$P(EXPERIENCE) = \\frac{count(EXPERIENCE)}{count(THINK) + count(EXPERIENCE)}$\n",
    "\n",
    "$P(EXPERIENCE) = \\frac{15}{12 + 15}$\n",
    "\n",
    "$P(EXPERIENCE) = \\frac{5}{9}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Naive Bayes: Computing the probability of some features by hand (10 points)\n",
    "\n",
    "The features appearing in the single test datapoint in `wsd_test.txt` are I,that,there,was,a,pain,in,my,leg. \n",
    "\n",
    "For each of these features, compute its conditional probability under each sense. That is, compute $P(I|THINK)$ and $P(I|EXPERIENCE)$, $P(that|THINK)$ and $P(that|EXPERIENCE)$, and so on for all the features in the test datapoint. **For this problem, please do the computation by hand.**\n",
    "\n",
    "To remind you how this works: While we cannot know, in the English language in general, how likely the\n",
    "word I is to appear given that we have an occurrence of feel in the THINK sense we can estimate the probabilities from the training data. For example, $P(I|THINK)$ would be estimated by concatenating all the THINK sentences into one long \"document\", and count how often \"I\" appears in it. Also compute how long this \"document\" is overall. Then the estimated probability is $\\frac{count_{THINK}(I)}{count_{THINK}(\\_)}$. You do not need to apply any smoothing.\n",
    "\n",
    "Again, please solve this problem by hand, and show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*\n",
    "\n",
    "# CLARIFY\n",
    "## What does \"compute how long the document is overall\" mean? Total sentences, total words?\n",
    "\n",
    "# ***THINK***\n",
    "\n",
    "$P(I|THINK) = \\frac{count_{THINK}(I)}{count_{THINK}(\\_)} = \\frac{6}{523} = 0.0114$\n",
    "\n",
    "$P(that|THINK) = \\frac{count_{THINK}(that)}{count_{THINK}(\\_)} = \\frac{7}{523} = 0.0133$\n",
    "\n",
    "$P(there|THINK) = \\frac{count_{THINK}(there)}{count_{THINK}(\\_)} = \\frac{1}{523} = 0.0019$\n",
    "\n",
    "$P(was|THINK) = \\frac{count_{THINK}(was)}{count_{THINK}(\\_)} = \\frac{2}{523} = 0.0038$\n",
    "\n",
    "$P(a|THINK) = \\frac{count_{THINK}(a)}{count_{THINK}(\\_)} = \\frac{1}{523} = 0.0019$\n",
    "\n",
    "$P(pain|THINK) = \\frac{count_{THINK}(pain)}{count_{THINK}(\\_)} = \\frac{1}{523} = 0.0019$\n",
    "\n",
    "$P(in|THINK) = \\frac{count_{THINK}(in)}{count_{THINK}(\\_)} = \\frac{1}{523} = 0.0019$\n",
    "\n",
    "$P(my|THINK) = \\frac{count_{THINK}(my)}{count_{THINK}(\\_)} = \\frac{1}{523} = 0.0019$\n",
    "\n",
    "$P(leg|THINK) = \\frac{count_{THINK}(leg)}{count_{THINK}(\\_)} = \\frac{1}{523} = 0.0019$\n",
    "\n",
    "\n",
    "# ***EXPERIENCE***\n",
    "\n",
    "$P(I|EXPERIENCE) = \\frac{count_{EXPERIENCE}(I)}{count_{EXPERIENCE}(\\_)} = \\frac{9}{457} = 0.0196$\n",
    "\n",
    "$P(that|EXPERIENCE) = \\frac{count_{EXPERIENCE}(that)}{count_{EXPERIENCE}(\\_)} = \\frac{2}{457} = 0.0043$\n",
    "\n",
    "$P(there|EXPERIENCE) = \\frac{count_{EXPERIENCE}(there)}{count_{EXPERIENCE}(\\_)} = \\frac{1}{457} = 0.0021$\n",
    "\n",
    "$P(was|EXPERIENCE) = \\frac{count_{EXPERIENCE}(there)}{count_{EXPERIENCE}(\\_)} = \\frac{1}{457} = 0.0021$\n",
    "\n",
    "$P(a|EXPERIENCE) = \\frac{count_{EXPERIENCE}(a)}{count_{EXPERIENCE}(\\_)} = \\frac{5}{457} = 0.0109$\n",
    "\n",
    "$P(pain|EXPERIENCE) = \\frac{count_{EXPERIENCE}(pain)}{count_{EXPERIENCE}(\\_)} = \\frac{4}{457} = 0.0087$\n",
    "\n",
    "$P(in|EXPERIENCE) = \\frac{count_{EXPERIENCE}(in)}{count_{EXPERIENCE}(\\_)} = \\frac{1}{457} = 0.0021$\n",
    "\n",
    "$P(my|EXPERIENCE) = \\frac{count_{EXPERIENCE}(my)}{count_{EXPERIENCE}(\\_)} = \\frac{4}{457} = 0.0087$\n",
    "\n",
    "$P(leg|EXPERIENCE) = \\frac{count_{EXPERIENCE}(leg)}{count_{EXPERIENCE}(\\_)} = \\frac{1}{457} = 0.0021$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Naive Bayes: Computing the most likely class label for a datapoint by hand (10 points)\n",
    "\n",
    "For this problem, you will again use the data from problems 2 and 3, this time to determine the most likely sense for the test item. The test item is \n",
    "    I *felt* that there was a pain in my leg.\n",
    "    \n",
    "Remember that in Naive Bayes, the most likely sense $c$ for datapoint $d$ is the one for which\n",
    "    $P(c) (d|c)$\n",
    "    \n",
    "is largest. Per the \"naive\" assumption, we compute the probability of the datapoint given the class, $P(d|c)$, as follows: If $d$ contains features $f_1, \\ldots, f_n$, then $P(d|c) \\approx \\prod_{i=1}^n P(f_i|c)$. Here $\\prod_{i=1}^n P(f_i(c)$ is  $P(f_1|c)$ multiplied with $P(f_2|c)$ multiplied with... and multiplied with $P(f_n|c)$. \n",
    "\n",
    "You will need to re-use your probabilities $P(THINK)$ and $P(EXPERIENCE)$ from problem 2, and the feature probabilities from problem 3.\n",
    "\n",
    "You will need to compute this value $P(c) \\prod_f P(f|c)$ both for $c=THINK$, and $c=EXPERIENCE$. The two values will not sum up to one -- remember that we dropped a denominator somewhere on the way. \n",
    "\n",
    "**For this problem, please do the calculations by hand, and show your work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*space for your text answer here*\n",
    "\n",
    "# CLARIFY\n",
    "\n",
    "# ***THINK***\n",
    "\n",
    "- $P(c) \\prod_f P(I\\:that\\:there\\:was\\:a\\:pain\\:in\\:my\\:leg|THINK) = P(I|THINK) \\cdot P(that|THINK) \\cdot P(there|THINK) \\cdot P(was|THINK) \\cdot P(a|THINK) \\cdot P(pain|THINK) \\cdot P(in|THINK) \\cdot P(my|THINK) \\cdot P(leg|THINK)$\n",
    "\n",
    "- $P(c) \\prod_f P(I\\:that\\:there\\:was\\:a\\:pain\\:in\\:my\\:leg|THINK) = 0.0114 \\cdot 0.0133 \\cdot 0.0019 \\cdot 0.0038 \\cdot 0.0019 \\cdot 0.0019 \\cdot 0.0019 \\cdot 0.0019 \\cdot 0.0019$\n",
    "\n",
    "- $P(c) \\prod_f P(I\\:that\\:there\\:was\\:a\\:pain\\:in\\:my\\:leg|THINK) = 2.7105766613436e^{-23}$\n",
    "\n",
    "# ***EXPERIENCE***\n",
    "\n",
    "- $P(c) \\prod_f P(I\\:felt\\:that\\:there\\:was\\:a\\:pain\\:in\\:my\\:leg|EXPERIENCE) = P(I|EXPERIENCE) \\cdot P(that|EXPERIENCE) \\cdot P(there|EXPERIENCE) \\cdot P(was|EXPERIENCE) \\cdot P(a|EXPERIENCE) \\cdot P(pain|EXPERIENCE) \\cdot P(in|EXPERIENCE) \\cdot P(my|EXPERIENCE) \\cdot P(leg|EXPERIENCE)$\n",
    "\n",
    "- $P(c) \\prod_f P(I\\:that\\:there\\:was\\:a\\:pain\\:in\\:my\\:leg|THINK) = 0.0196 \\cdot 0.0043 \\cdot 0.0021 \\cdot 0.0021 \\cdot 0.0109 \\cdot 0.0087 \\cdot 0.0021 \\cdot 0.0087 \\cdot 0.0021$\n",
    "\n",
    "- $P(c) \\prod_f P(I\\:that\\:there\\:was\\:a\\:pain\\:in\\:my\\:leg|THINK) = 1.3522802619032273e^{-21}$\n",
    "\n",
    "### **The most likely sense for the test item is EXPERIENCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 Naive Bayes: Computing the probability of each target class with Python (10 pts.)\n",
    "\n",
    "For this problem, please answer the same question as in problem 2, but this time do it with Python.\n",
    "\n",
    "Here is the training data again, this time as Python data structures. Here is a list of the training sentences, with the :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feel_training_data = [\n",
    "    'I began to some guilt', \n",
    "    'I sorry for you Nardo', \n",
    "    'I the surge of adrenaline that comes right when your body realizes that its too late to save itself', \n",
    "    'It just like it was before', \n",
    "    'I still a little woozy', \n",
    "    'I pain under my jaw', \n",
    "    'I a tug on the back of my shirt', \n",
    "    'then I the pain withdraw and a lifeless voice said', \n",
    "    'I a warm touch on my cheek', \n",
    "    'I my leg turn', \n",
    "    'there is no need to troubled', \n",
    "    'he sorry for them', \n",
    "    'suddenly she a sharp pain in her jaw', \n",
    "    'Ms. Schaefer intense pain', \n",
    "    'yet we no longer uneasy', \n",
    "    'I like I was a big part of the team', \n",
    "    'they got help where they they needed it most', \n",
    "    'the team that they could part with the player', \n",
    "    'she that many pain specialists receive inadequate training', \n",
    "    'they that the police could not guarantee their safety', \n",
    "    'I have always that I fitted in there', \n",
    "    'your doctor that leg surgery is the best way to treat it', \n",
    "    \"many people have that they don't have any power\", \n",
    "    'I that I had so much to prove to my team', \n",
    "    'we like it was starting to be taken from us', \n",
    "    \"Jack he is hurting Boston's chances\", \n",
    "    'he this is the least risky path'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the gold labels for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feel_training_labels = ['EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'THINK', 'THINK', 'THINK', 'THINK', 'THINK', \n",
    "                        'THINK', 'THINK', 'THINK', 'THINK', 'THINK',\n",
    "                        'THINK', 'THINK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please again compute $P(THINK)$ and $P(EXPERIENCE)$, but **this time use Python code to do so**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 55.556%\n",
      "Think: 44.444%\n"
     ]
    }
   ],
   "source": [
    "# please place your code here\n",
    "\n",
    "# check if this is the same\n",
    "# if you did problem 2 right, this is right - vice versa\n",
    "\n",
    "\n",
    "#check this, should the sum be total words or total sentences in the corpus?\n",
    "sum = len(feel_training_labels)\n",
    "\n",
    "p_think = \"{:.3%}\".format(len([label for label in feel_training_labels if label == \"THINK\"])/sum)\n",
    "\n",
    "p_exp = \"{:.3%}\".format(len([label for label in feel_training_labels if label == \"EXPERIENCE\"])/sum)\n",
    "\n",
    "print(f\"Experience: {p_exp}\\nThink: {p_think}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Naive Bayes: Computing the probability of some features using Python (10 points)\n",
    "\n",
    "For this problem, please answer the same question as for problem 3, but this time please do this in Python. \n",
    "\n",
    "Here is the relevant list of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [\"I\", \"that\", \"there\", \"was\", \n",
    "                 \"a\", \"pain\", \"in\", \"my\",  \"leg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature in `test_features`, please compute its probability \n",
    "given THINK, and also its probability given EXPERIENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXP:\n",
      "'I': 0.0197\n",
      "'that': 0.00438\n",
      "'there': 0.00219\n",
      "'was': 0.00219\n",
      "'a': 0.0109\n",
      "'pain': 0.00875\n",
      "'in': 0.00219\n",
      "'my': 0.00875\n",
      "'leg': 0.00219\n",
      "\n",
      "\n",
      "THINK:\n",
      "'I': 0.0115\n",
      "'that': 0.0134\n",
      "'there': 0.00191\n",
      "'was': 0.00382\n",
      "'a': 0.00191\n",
      "'pain': 0.00191\n",
      "'in': 0.00191\n",
      "'my': 0.00191\n",
      "'leg': 0.00191\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# space for your code here\n",
    "\n",
    "# get the total word counts for each class\n",
    "\n",
    "sum_dict = {\"EXP\": 0 , \"THINK\": 0}\n",
    "\n",
    "for i in range(len(feel_training_data)):\n",
    "    if feel_training_labels[i] == \"EXPERIENCE\":\n",
    "        sum_dict[\"EXP\"] += len(feel_training_data[i])\n",
    "    else:\n",
    "        sum_dict[\"THINK\"] += len(feel_training_data[i])\n",
    "\n",
    "# created a nested dictionary with one entry for experience and one for think\n",
    "# the inner dictionaries will have a key value pair for each word in test_features\n",
    "\n",
    "words_dict = {\"EXP\": {word: 0 for word in test_features}, \"THINK\": {word: 0 for word in test_features}}\n",
    "\n",
    "# loop through each sentence and each word in the sentence\n",
    "# keep the first loop as a range to be able to cross reference labels\n",
    "\n",
    "for i in range(len(feel_training_data)):\n",
    "    for word in feel_training_data[i].split(\" \"):\n",
    "        if feel_training_labels[i] == \"EXPERIENCE\":\n",
    "            if word in words_dict[\"EXP\"]:\n",
    "                words_dict[\"EXP\"][word] += 1\n",
    "\n",
    "        else:\n",
    "            if word in words_dict[\"THINK\"]:\n",
    "                words_dict[\"THINK\"][word] += 1\n",
    "\n",
    "prob_dict = {\"EXP\": {word: 0 for word in test_features}, \"THINK\": {word: 0 for word in test_features}}\n",
    "\n",
    "# find the probability of a word by dividing the number of occurences in a given sense by the total words in the sense overall\n",
    "# add that probability to the prob_dict for later use\n",
    "# print probabilities under each sense\n",
    "\n",
    "for key, value in words_dict.items():\n",
    "    print(key + \":\")\n",
    "\n",
    "    for word in test_features:\n",
    "\n",
    "        prob = \"{:.3}\".format(words_dict[key][word]/sum_dict[key])\n",
    "\n",
    "        prob_dict[key][word] = prob\n",
    "\n",
    "        print(f\"\\'{word}\\': {prob}\")\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Naive Bayes: Computing the most likely class label for a datapoint with Python (10 points)\n",
    "\n",
    "For this problem, you need to again answer the same question as in problem 4, but this time find the solution using Python code: For the one datapoint in `wsd_test.txt`, which is the likeliest class, THINK or EXPERIENCE?\n",
    "\n",
    "For the test datapoint $d$, please compute both $P(THINK) P(d|THINK)$ and $P(EXPERIENCE) P(d|EXPERIENCE)$. Then say which one is bigger. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The likeliest class is EXPERIENCE.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create a list for each sense\n",
    "# to hold all of the probabilities\n",
    " \n",
    "exp_list = [float(prob_dict[key][word]) for word in prob_dict[key] for key in prob_dict if key == \"EXP\"]\n",
    "\n",
    "think_list = [float(prob_dict[key][word]) for word in prob_dict[key] for key in prob_dict if key == \"THINK\"]\n",
    "\n",
    "# use numpy prod to multiply all the elements of each list\n",
    "\n",
    "exp_res = np.prod(exp_list)\n",
    "think_res = np.prod(think_list)\n",
    "\n",
    "# check which is bigger and print the result\n",
    "# the results vary slightly from problem 4 because the floating point values used for this computation\n",
    "# are more precise than the ones I used to compute the probability by hand\n",
    "# however, the result is still the same regardless of the small differences\n",
    "\n",
    "if exp_res > think_res:\n",
    "    print(\"The likeliest class is EXPERIENCE.\")\n",
    "else:\n",
    "    print(\"The likeliest class is THINK.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c27904bfabd9e4a7f1d5f5dcc5d2a6d0fab6e51da12729af85e6eafdbb7b7a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
