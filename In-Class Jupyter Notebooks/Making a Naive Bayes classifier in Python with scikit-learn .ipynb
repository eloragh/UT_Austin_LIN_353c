{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a toy example of how to do machine learning with scikit-learn, adapted from the scikit-learn tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the toy data\n",
    "X = [[ 1,  2,  3],  # 2 samples, 3 features\n",
    "     [11, 12, 13]]\n",
    "\n",
    "y = [0, 1]  # classes of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will train a Naive Bayes classifier on the training data in X,\n",
    "# using the gold labels in y\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# When we make a NaiveBayes classifier without changing any \n",
    "# of the default parameter values, it does\n",
    "# add-one smoothing, that's what \"allpha = 1.0\" means.\n",
    "# class_prior = None means that we're not setting P(c)\n",
    "# for the outcome classes c ahead of seeing the data,\n",
    "# and fit_prior=True means that we're compuring P(c)\n",
    "# from the training data\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction on the training data:\n",
    "# We replicate the training data, predicting a label of 0\n",
    "# for the first datapoint, and 0 for the second datapoint.\n",
    "# But then, testing on the training data does not tell us much.\n",
    "nb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is data that the model has not previously seen\n",
    "testdata = [\n",
    "    [4, 5, 6], \n",
    "    [14, 15, 16]]\n",
    "\n",
    "# The model predicts label 1 for both datapoints.\n",
    "nb.predict(testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move to an NLP task: We do the mini sentiment analysis task from Jurafsky and Martin 3rd edition chapter 4. Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mini corpus from J&M version 3\n",
    "jm_corpus = [\n",
    "    'just plain boring',\n",
    "    'entirely predictable and lacks energy',\n",
    "    'no surprises and very few laughs',\n",
    "    'very powerful',\n",
    "    'the most fun film of the summer'\n",
    "    ]\n",
    "# gold labels on these documents\n",
    "jm_y = [0, 0, 0, 1, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn needs its data to be in a different format: a vector of one entry per feature (in our case, word type), with a value for each feature. In our case, these values are the word counts.\n",
    "\n",
    "scikit-learn comes with a number of predefined data structures that transform into its input format. There is even a feature extractor specifically for word counts, the CountVectorizer. Here it is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# make an object that does the feature extraction\n",
    "counting_feature_extractor = CountVectorizer()\n",
    "\n",
    "# fit_transform does two things: \n",
    "# 1. it checks which words are in this corpus, and assumes that these are \n",
    "#   all the words/features the model will pay attention to.\n",
    "# 2. It transforms the corpus into a matrix of counts\n",
    "jm_X = counting_feature_extractor.fit_transform(jm_corpus)\n",
    "jm_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and, boring, energy, entirely, few, film, fun, just, lacks, laughs, most, no, of, plain, powerful, predictable, summer, surprises, the, very\n"
     ]
    }
   ],
   "source": [
    "# Here is a list of all the words that the feature extractor \n",
    "# has seen:\n",
    "print(\", \".join(counting_feature_extractor.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a closer look at the matrix of counts.\n",
    "# the only word ever to appear twice in a corpus is \"the\", \n",
    "# in the last document\n",
    "jm_X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can train a Naive Bayes model on this data as before\n",
    "nb_jm = MultinomialNB()\n",
    "nb_jm.fit(jm_X, jm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x20 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We test this on the test corpus\n",
    "# (consisting of just one document)\n",
    "# from the Jurafsky and Martin example\n",
    "jm_testcorpus = [ 'predictable with no fun']\n",
    "\n",
    "# To transform the datapoint into a feature representation,\n",
    "# we just call \"transform\", not \"fit_transform\":\n",
    "# The feature extractor has been fitted to the training data,\n",
    "# and is now applied to the test data,\n",
    "# meaning that it only pays attention to the words\n",
    "# that appeared in the training data. \n",
    "jm_testX = counting_feature_extractor.transform(jm_testcorpus)\n",
    "jm_testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspecting the matrix of counts for the test data\n",
    "jm_testX.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the model predicts for this test datapoint\n",
    "nb_jm.predict(jm_testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn has many other feature extractors. There is also one that assumes that each data point is given as a Python dictionary of the form\n",
    "\n",
    "```{feature1:value1, feature2:value2, ...}```\n",
    "\n",
    "so that a whole dataset is given as a list of such dictionaries. \n",
    "\n",
    "We can transform our data into this format ourselves if we like, using NLTK's language processing functions for the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'just': 1, 'plain': 1, 'boring': 1}),\n",
       " FreqDist({'entirely': 1, 'predictable': 1, 'and': 1, 'lacks': 1, 'energy': 1}),\n",
       " FreqDist({'no': 1, 'surprises': 1, 'and': 1, 'very': 1, 'few': 1, 'laughs': 1}),\n",
       " FreqDist({'very': 1, 'powerful': 1}),\n",
       " FreqDist({'the': 2, 'most': 1, 'fun': 1, 'film': 1, 'of': 1, 'summer': 1})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "jm_corpus_countdict = [ ]\n",
    "for document in jm_corpus:\n",
    "    doc_preprocessed = nltk.word_tokenize(document)\n",
    "    doc_countdict = nltk.FreqDist(doc_preprocessed)\n",
    "    jm_corpus_countdict.append(doc_countdict)\n",
    "    \n",
    "jm_corpus_countdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now use a feature extractor that can deal with\n",
    "# data points that are count dictionaries\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dict_feature_extractor = DictVectorizer()\n",
    "jm_X2 = dict_feature_extractor.fit_transform(jm_corpus_countdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 2., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jm_X2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now again train a classifier\n",
    "nb_jm2 = MultinomialNB()\n",
    "nb_jm2.fit(jm_X2, jm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'predictable': 1, 'with': 1, 'no': 1, 'fun': 1})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we preprocess the test corpus in the same way\n",
    "jm_testcorpus_countdict = [ ]\n",
    "for document in jm_testcorpus:\n",
    "    doc_preprocessed = nltk.word_tokenize(document) \n",
    "    doc_dict = nltk.FreqDist(doc_preprocessed)\n",
    "    jm_testcorpus_countdict.append(doc_dict)\n",
    "    \n",
    "jm_testcorpus_countdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming this data into a matrix\n",
    "jm_testX2 = dict_feature_extractor.transform(jm_testcorpus_countdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and handing it to the Naive Bayes classifier to classify\n",
    "nb_jm2.predict(jm_testX2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has labeled the single test datapoint correctly: It has negative sentiment. \n",
    "\n",
    "But interestingly, if we remove stopwords from this data, it will mis-classify the test datapoint as having positive sentiment -- because \"no\" is a stopword!\n",
    "\n",
    "**Try it for yourself**: Here is the first step, the preprocessing of the training data, using variable names starting in \"sw\" for \"stopwords\". Can you do the rest? You will need to:\n",
    "\n",
    "* preprocess the training data into dictionaries of counts (already done)\n",
    "* transform the training data into a matrix of feature values\n",
    "* train a Naive Bayes classifier with the data\n",
    "* preprocess the test data into a dictionary of counts\n",
    "* transform the test data into a matrix of feature values\n",
    "* use `predict()` to see what class the Naive Bayes classifier assigns to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'plain': 1, 'boring': 1}),\n",
       " FreqDist({'entirely': 1, 'predictable': 1, 'lacks': 1, 'energy': 1}),\n",
       " FreqDist({'surprises': 1, 'laughs': 1}),\n",
       " FreqDist({'powerful': 1}),\n",
       " FreqDist({'fun': 1, 'film': 1, 'summer': 1})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "\n",
    "sw_jm_corpus_countdict = [ ]\n",
    "for document in jm_corpus:\n",
    "    doc_preprocessed = [w for w in nltk.word_tokenize(document) \n",
    "                        if w not in en_stopwords]\n",
    "    sw_jm_corpus_countdict.append(nltk.FreqDist(doc_preprocessed))\n",
    "    \n",
    "sw_jm_corpus_countdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
