{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with bigram language models\n",
    "\n",
    "This notebook uses the NLTK LanguageModel data type and its subtype MLE, a language model estimated using relative corpus frequencies as word probabilities. \n",
    "\n",
    "This notebook is not intended as a demo of how to implement language models from scratch but as a demo of language model output.\n",
    "\n",
    "## The \"Sam corpus\"\n",
    "\n",
    "Our first example uses the Green Eggs and Ham poem as a \"corpus\" from which we train a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus as a list of sentences: ['I am Sam.', 'Sam I am.', 'I do not like green eggs and ham.']\n",
      "Vocabulary: ['I', 'do', 'and', 'Sam', 'not', 'eggs', 'green', '.', 'am', 'ham', 'like']\n",
      "Corpus as a list of sentences, each of which\n",
      "is a list of bigrams: [[('I', 'am'), ('am', 'Sam'), ('Sam', '.')], [('Sam', 'I'), ('I', 'am'), ('am', '.')], [('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham'), ('ham', '.')]]\n",
      "Corpus as a list of sentences, each of which\n",
      "is a list of unigrams: [[('I',), ('am',), ('Sam',), ('.',)], [('Sam',), ('I',), ('am',), ('.',)], [('I',), ('do',), ('not',), ('like',), ('green',), ('eggs',), ('and',), ('ham',), ('.',)]]\n"
     ]
    }
   ],
   "source": [
    "# we import the natural language toolkit\n",
    "import nltk\n",
    "# and its maximum likelihood estimation (MLE) language model\n",
    "from nltk.lm import MLE\n",
    "\n",
    "# lm is now an object of type MLE, in particular\n",
    "# a bigram language model, because we put the parameter 2\n",
    "lm_sam = MLE(2)\n",
    "\n",
    "# Here is our corpus, first as plain text\n",
    "text = \"I am Sam. Sam I am. I do not like green eggs and ham.\"\n",
    "# ... and here it is cut up into sentences\n",
    "text_sents = nltk.sent_tokenize(text)\n",
    "# ... and cut up into words\n",
    "text_words = nltk.word_tokenize(text)\n",
    "# ... and cut up into sentences, each of which\n",
    "# is cut up into words\n",
    "text_sent_words = [ nltk.word_tokenize(s) for s in text_sents]\n",
    "\n",
    "print(\"Corpus as a list of sentences:\", text_sents)\n",
    "\n",
    "# The language model needs to know the vocabulary.\n",
    "# We turn the list of words into a set, which eliminates\n",
    "# duplicates, then turn that back into a list\n",
    "vocab = list(set(text_words))\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# The language model needs to be trained\n",
    "# with both bigrams and unigrams. \n",
    "# We start with the bigrams,\n",
    "# turning each sentence of the text\n",
    "# into a list of bigrams\n",
    "text_bigrams = [ list(nltk.bigrams(s)) for s in text_sent_words]\n",
    "print(\"Corpus as a list of sentences, each of which\")\n",
    "print(\"is a list of bigrams:\", text_bigrams)\n",
    "\n",
    "# The method fit() trains the language model on the text.\n",
    "# The first time we call fit(), we need to also pass\n",
    "# it the vocabulary, or it will raise an error.\n",
    "lm_sam.fit(text_bigrams, vocabulary_text = vocab)\n",
    "\n",
    "# Now we train with unigrams.\n",
    "# We turn each sentence of the text into a list of unigrams\n",
    "text_unigrams = [[(w,) for w in s] for s in text_sent_words]\n",
    "print(\"Corpus as a list of sentences, each of which\")\n",
    "print(\"is a list of unigrams:\", text_unigrams)\n",
    "# Then we use fit() again. Since we have used it before, \n",
    "# we do not need to pass it the vocabulary again.\n",
    "lm_sam.fit(text_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Sam I do not like green eggs and ham . green eggs and ham . I am . am "
     ]
    }
   ],
   "source": [
    "# Now we use the language model to generate text.\n",
    "# The first word is \"I\". We print it.\n",
    "currentword = \"I\"\n",
    "print(currentword, end = \" \")\n",
    "# The language model knows, for each word w, \n",
    "# the conditional probability P(w|currentword).\n",
    "# We use that to generate text:\n",
    "# lm.generate() picks a word to follow currentword\n",
    "# It does not always pick the most likely one,\n",
    "# But it will be more likely to pick a more likely following word:\n",
    "for i in range(20):\n",
    "    currentword = lm_sam.generate(text_seed= [currentword])\n",
    "    print(currentword,end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the language model does something weird at sentence boundaries: It picks words to start a sentence that have never been observed at the start of a sentence. \n",
    "\n",
    "If we want the language model to behave exactly as in the book, we can do that by giving it the whole text as a single \"sentence\", but using sentence boundary markers ``<s>`` and ``</s>``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus as a list of words: ['<s>', 'I', 'am', 'Sam', '</s>', '<s>', 'Sam', 'I', 'am', '</s>', '<s>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '</s>']\n",
      "Vocabulary: ['I', 'do', 'and', '<s>', 'not', 'eggs', 'green', 'am', 'Sam', 'ham', 'like', '</s>']\n",
      "Corpus as bigrams: [[('<s>', 'I'), ('I', 'am'), ('am', 'Sam'), ('Sam', '</s>'), ('</s>', '<s>'), ('<s>', 'Sam'), ('Sam', 'I'), ('I', 'am'), ('am', '</s>'), ('</s>', '<s>'), ('<s>', 'I'), ('I', 'do'), ('do', 'not'), ('not', 'like'), ('like', 'green'), ('green', 'eggs'), ('eggs', 'and'), ('and', 'ham'), ('ham', '</s>')]]\n",
      "Corpus as unigrams: [[('<s>',), ('I',), ('am',), ('Sam',), ('</s>',), ('<s>',), ('Sam',), ('I',), ('am',), ('</s>',), ('<s>',), ('I',), ('do',), ('not',), ('like',), ('green',), ('eggs',), ('and',), ('ham',), ('</s>',)]]\n"
     ]
    }
   ],
   "source": [
    "# Here is the corpus again.\n",
    "text = \"<s> I am Sam </s> <s> Sam I am </s> <s> I do not like green eggs and ham </s>\"\n",
    "# This time we don't make sentences\n",
    "# and we use split() to split, so it does not \n",
    "# cut up our sentence start and end markers.\n",
    "text_words = text.split()\n",
    "print(\"Corpus as a list of words:\", text_words)\n",
    "\n",
    "# we make another bigram language model\n",
    "lm_sam2 = MLE(2)\n",
    "\n",
    "# vocabulary: words in the text, without repetitions\n",
    "vocab = list(set(text_words))\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# training input: list of sentences,\n",
    "# where each sentence is a tuple of bigrams\n",
    "text_bigrams = [ list(nltk.bigrams(text_words)) ]\n",
    "print(\"Corpus as bigrams:\", text_bigrams)\n",
    "\n",
    "lm_sam2.fit(text_bigrams, vocabulary_text = vocab)\n",
    "\n",
    "# now let's do unigrams\n",
    "text_unigrams = [[(w,) for w in text_words] ]\n",
    "print(\"Corpus as unigrams:\", text_unigrams)\n",
    "lm_sam2.fit(text_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I am </s> <s> I am </s> <s> I am Sam I do not like green eggs and ham </s> <s> I am Sam </s> <s> Sam </s> <s> I "
     ]
    }
   ],
   "source": [
    "# We generate once more, this time starting with\n",
    "# the sentence start marker\n",
    "word = \"<s>\"\n",
    "print(word, end = \" \")\n",
    "for i in range(30):\n",
    "    word = lm_sam2.generate(text_seed= [word])\n",
    "    print(word,end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inaugural addresses\n",
    "\n",
    "We now use a larger corpus: the corpus of inaugural addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try training an inaugural speeches language model\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "# Getting the inaugural addresses corpus\n",
    "# To run this on a different corpus,\n",
    "# just switch out the definitions of corpus_sents\n",
    "# and corpus_words, everything else stays the same\n",
    "\n",
    "# Our corpus, cut up into sentences\n",
    "corpus_sents = inaugural.sents()\n",
    "# and cut up into words\n",
    "corpus_words = inaugural.words()\n",
    "\n",
    "# Each sentence turned into a list of bigrams\n",
    "bigrams = [list(nltk.bigrams(s)) for s in corpus_sents]\n",
    "\n",
    "# making a language model, bigram again\n",
    "lm_inaug = MLE(2)\n",
    "\n",
    "# vocabulary: a list of all words \n",
    "# in the corpus, without duplicates\n",
    "vocab = list(set(corpus_words))\n",
    "\n",
    "# training with the bigrams\n",
    "lm_inaug.fit(bigrams, vocabulary_text = vocab)\n",
    "\n",
    "# and training with the unigrams\n",
    "unigrams = [[(w,) for w in s] for s in corpus_sents]\n",
    "lm_inaug.fit(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can not attain their place where it or to the policy of the same glorious beauty ; and usefulness ends of those who are worse in each generation -- until it . Jefferson , and no new States have few leading strings , for their exposition of nations which there "
     ]
    }
   ],
   "source": [
    "# Now we generate from the inaugural language model\n",
    "word = lm_inaug.generate()\n",
    "print(word, end = \" \")\n",
    "for i in range(50):\n",
    "    word = lm_inaug.generate(text_seed= [word])\n",
    "    print(word,end = \" \")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
