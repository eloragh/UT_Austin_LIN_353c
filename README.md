# [LIN 353c Intro to Computational Linguistics](https://sites.google.com/utexas.edu/lin353c-introduction-to-comput/home) <img align="right" width="150" height="150" src=Misc/UT_Seal.png>

## Table of Contents

- [Introduction](#introduction)
- [In-Class Notebooks](#in-class-notebooks)
- [Assignments](#assignments)
- [Acknowledgements](#acknowledgements)

## Introduction
This repository contains all of the assignments and input text files for LIN 353c Introduction to Computational Linguistics with Professor Katrin Erk in the Fall 2022 semester at the University of Texas at Austin. This course covers the intersection of linguistics and computer science through machine learning algorithms and dynamic programming.

## [In-Class Notebooks](https://github.com/eloragh/LING_353c_COMP_LING/tree/main/In-Class%20Jupyter%20Notebooks)
The Jupyter Notebooks used in-class covered a variety of topics from simple python (data structures, loops, conditionals, imports, files) to the implementation of sophisticated machine learning models and algorithms.

## [Assignments](https://github.com/eloragh/LING_353c_COMP_LING/tree/main/Homework)

[Homework 1](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_1_espie.ipynb)
  - Python variables, string slicing, string methods, regular expressions, stemming, and anonymization.
  
[Homework 2](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_2_espie.ipynb)
  - For loops, n-gram language models, part-of-speech tagging, word frequency, tag frequency, and part-of-speech ambiguity.

[Homework 3](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_3%20_espie.ipynb)
  - Trigram language model statistical proofs, maximum likelihood estimation (MLE), Laplace smoothing, and nearest neighbor classification.
 
[Homework 4](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_4_espie.ipynb)
  - Word sense disambiguation, Naive Bayes statistical proofs, and Naive Bayes machine learning model implementation.
  
[Homework 5](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_5_espie.ipynb)
  - Sentiment analysis, logistic regression models, gensim, sklearn, numpy, matplotlib, plotting nearest neighbor vectors, cosine similarity computation.

[Homework 6](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_6_espie.ipynb)
  - Part-of-speech tagging disambiguation, [Crash Blossoms](https://www.nytimes.com/2010/01/31/magazine/31FOB-onlanguage-t.html), Hidden Markov models,
  and Viterbi algorithm.

[Homework 7](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_7_espie.ipynb)
  - Chunking, precision, recall, context-free grammar.
 
[Homework 8](https://github.com/eloragh/LING_353c_COMP_LING/blob/main/Homework/Introcl_homework_8_espie.ipynb)
  - Cocke-Younger-Kasami (CKY) algorithm.

## Acknowledgements

Computational Linguistics has been my desired trajectory for a long time. Taking this class felt like the first real step to fulfilling a long awaited dream. Professor Erk is a fantastic teacher who continued to support my final project long after the class had finished. She introduced me to the broader Computational Linguistics community at UT Austin, providing me with the opportunity to continue working towards my goal.

Thanks for checking out my repository!
